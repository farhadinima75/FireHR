{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "> This module contains functions to download and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ee\n",
    "import os\n",
    "import requests\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from IPython.core.debugger import set_trace\n",
    "from pathlib import Path\n",
    "from banet.geo import open_tif, merge, Region\n",
    "from banet.geo import downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Region(Region):\n",
    "    @property\n",
    "    def transform(self):\n",
    "        \"Rasterio Affine transform of the region\"\n",
    "        return rasterio.transform.from_origin(self.bbox[0], self.bbox[-1], \n",
    "                                              self.pixel_size, self.pixel_size)\n",
    "    \n",
    "class RegionST(Region):\n",
    "    \"Defines a region in space and time with a name, a bounding box and the pixel size.\"\n",
    "    def __init__(self, name:str, bbox:list, pixel_size:float, time_start:str=None,\n",
    "                 time_end:str=None, time_freq:str='D', time_margin:int=0):\n",
    "        self.name = name\n",
    "        self.bbox = rasterio.coords.BoundingBox(*bbox) # left, bottom, right, top\n",
    "        self.pixel_size = pixel_size\n",
    "        self.time_start = pd.Timestamp(str(time_start))\n",
    "        self.time_end = pd.Timestamp(str(time_end))\n",
    "        self.time_margin = time_margin\n",
    "        self.time_freq = time_freq\n",
    "\n",
    "    @property\n",
    "    def times(self):\n",
    "        \"Property that computes the date_range for the region.\"\n",
    "        tstart = self.time_start - pd.Timedelta(days=self.time_margin)\n",
    "        tend = self.time_end + pd.Timedelta(days=self.time_margin)\n",
    "        return pd.date_range(tstart, tend, freq=self.time_freq)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file, time_start=None, time_end=None):\n",
    "        \"Loads region information from json file\"\n",
    "        with open(file, 'r') as f:\n",
    "            args = json.load(f)\n",
    "        if time_start is None:\n",
    "            time_start = args['time_start']\n",
    "        if time_end is None:\n",
    "            time_end = args['time_end']\n",
    "        return cls(args['name'], args['bbox'], args['pixel_size'],\n",
    "                   time_start, time_end)\n",
    "    \n",
    "def extract_region(df_row, cls=Region):\n",
    "    \"Create Region object from a row of the metadata dataframe.\"\n",
    "    if issubclass(cls, RegionST):\n",
    "        return cls(df_row.event_id, df_row.bbox, df_row.pixel_size, \n",
    "                   df_row.time_start, df_row.time_end)\n",
    "    elif issubclass(cls, Region):\n",
    "        return cls(df_row.event_id, df_row.bbox, df_row.pixel_size)\n",
    "    else: raise NotImplemented('cls must be one of the following [Region, RegionST]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def coords2bbox(lon, lat, pixel_size): \n",
    "    return [lon.min(), lat.min(), lon.max()+pixel_size, lat.max()+pixel_size]\n",
    "\n",
    "def split_region(region:RegionST, size:int, cls=Region):\n",
    "    lon, lat = region.coords()\n",
    "    Nlon = (len(lon)//size)*size\n",
    "    Nlat = (len(lat)//size)*size\n",
    "    lons = [*lon[:Nlon].reshape(-1, size), lon[Nlon:][None]]\n",
    "    lats = [*lat[:Nlat].reshape(-1, size), lat[Nlat:][None]]\n",
    "    if len(lats[-1].reshape(-1)) == 0 and len(lons[-1].reshape(-1)) == 0:\n",
    "        lons = lons[:-1]\n",
    "        lats = lats[:-1]\n",
    "    #lons = lon.reshape(-1, size)\n",
    "    #lats = lat.reshape(-1, size)\n",
    "    if issubclass(cls, RegionST):\n",
    "        return [cls('', coords2bbox(ilon, ilat, region.pixel_size), \n",
    "                    pixel_size=region.pixel_size, time_start=region.time_start,\n",
    "                    time_end=region.time_end, time_freq=region.time_freq,\n",
    "                    time_margin=region.time_margin) for ilon in lons for ilat in lats]\n",
    "    elif issubclass(cls, Region):\n",
    "        return [cls('', coords2bbox(ilon, ilat, region.pixel_size), pixel_size=region.pixel_size) \n",
    "            for ilon in lons for ilat in lats]\n",
    "    else: raise NotImplemented('cls must be one of the following [Region, RegionST]')\n",
    "        \n",
    "    return \n",
    "            \n",
    "def merge_tifs(files:list, fname:str, delete=False):\n",
    "    data, tfm = merge([open_tif(str(f)) for f in files])\n",
    "    data = data.squeeze()\n",
    "    fname = Path(files[0]).parent/fname\n",
    "    profile = open_tif(str(files[0])).profile\n",
    "    with rasterio.Env():\n",
    "        height, width = data.shape\n",
    "        profile.update(width=width, height=height, transform=tfm, compress='lzw')\n",
    "        with rasterio.open(str(fname), 'w', **profile) as dst:\n",
    "            dst.write(data, 1)\n",
    "    if delete:\n",
    "        for f in files: os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_region(image_collection:ee.ImageCollection, region:RegionST, times:tuple, bands=None):\n",
    "    image_collection = image_collection.filterDate(times[0], times[1])\n",
    "    geometry = ee.Geometry.Rectangle(region.bbox)\n",
    "    image_collection = image_collection.filterBounds(geometry)\n",
    "    if bands is not None:\n",
    "        image_collection = image_collection.select(bands)\n",
    "    return image_collection\n",
    "\n",
    "def download_data(R:RegionST, times, products, bands, path_save, scale=10):\n",
    "    ee.Initialize()\n",
    "    path_save.mkdir(exist_ok=True, parents=True)\n",
    "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
    "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
    "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
    "        sR = [R] if R.shape[0] <= 32 else split_region(R, size=32, cls=RegionST)\n",
    "        fsaves = []\n",
    "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
    "        for j, R in enumerate(sR):\n",
    "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" +\n",
    "                       f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
    "\n",
    "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
    "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
    "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
    "                # Merge products to single image collection\n",
    "                imCol = ee.ImageCollection(products[0])\n",
    "                for i in range(1, len(products)):\n",
    "                    imCol = imCol.merge(ee.ImageCollection(products[i]))\n",
    "                im = filter_region(imCol, R, times=times, bands=bands).median()\n",
    "                imCol = ee.ImageCollection([im])\n",
    "                colList = imCol.toList(imCol.size())\n",
    "                # Download each image\n",
    "                for i in range(colList.size().getInfo()):\n",
    "                    image = ee.Image(colList.get(i))\n",
    "                    fname = 'download'\n",
    "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
    "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
    "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
    "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
    "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
    "                    if not fnames_full:\n",
    "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
    "                        if not fnames_partial:\n",
    "                            zip_error = True\n",
    "                            for i in range(10): # Try 10 times\n",
    "                                if zip_error:\n",
    "                                    try:\n",
    "                                        url = image.getDownloadURL(\n",
    "                                            {'scale': scale, 'crs': 'EPSG:4326', \n",
    "                                             'region': f'{region}'})\n",
    "                                        r = requests.get(url)\n",
    "                                        with open(str(path_save/'data.zip'), 'wb') as f:\n",
    "                                            f.write(r.content)\n",
    "                                        with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
    "                                            files = f.namelist()\n",
    "                                            f.extractall(str(path_save))\n",
    "                                        os.remove(str(path_save/'data.zip'))\n",
    "                                        zip_error = False\n",
    "                                    except:\n",
    "                                        zip_error = True\n",
    "                                        os.remove(str(path_save/'data.zip'))\n",
    "                                        time.sleep(10)\n",
    "                            if zip_error: raise Exception(f'Failed to process {url}')\n",
    "                            for f in files:\n",
    "                                f = path_save/f\n",
    "                                os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
    "        # Merge files\n",
    "        suffix = '.tif'\n",
    "        files = path_save.ls(include=[suffix])\n",
    "        #files = np.unique(fsaves) \n",
    "        files = [o.stem for o in files]\n",
    "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
    "                         for o in files if len(o.split('_')[-1]) < 6])\n",
    "        ids = np.unique([int(o.split('_')[-1]) \n",
    "                         for o in files if len(o.split('_')[-1]) < 6])\n",
    "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
    "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
    "                    if f'{r}_{i}' in files] for r in ref] \n",
    "        for fs in file_groups:\n",
    "            if len(fs) < 500:\n",
    "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
    "                merge_tifs(fs, fsave, delete=True)\n",
    "            else:\n",
    "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
    "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
    "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
    "                for fsi, fs2 in enumerate(fs_break):\n",
    "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
    "                    merge_tifs(fs2, fsave, delete=True)\n",
    "\n",
    "        files = path_save.ls(include=[suffix, '_break'])\n",
    "        files = [o.stem for o in files]\n",
    "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
    "                         for o in files if len(o.split('_')[-1]) < 11])\n",
    "        ids = np.unique([o.split('_')[-1]\n",
    "                         for o in files if len(o.split('_')[-1]) < 11])\n",
    "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
    "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
    "                    if f'{r}_{i}' in files] for r in ref] \n",
    "        for fs in file_groups:\n",
    "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
    "            merge_tifs(fs, fsave, delete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data for any region example:\n",
    "```python\n",
    "R = RegionST('test_region', [-9.0,39.95,-8.9,40.05], 0.001, time_start='2020-07-01', time_end='2020-07-15')\n",
    "R.time_margin=1\n",
    "products = [\"COPERNICUS/S2\"]\n",
    "bands = ['B4', 'B8', 'B12']\n",
    "path  = Path('temp')\n",
    "before = (R.times[0]-pd.Timedelta(days=60), R.times[0])\n",
    "after  = (R.times[-1], R.times[-1]+pd.Timedelta(days=60))\n",
    "for mode, time_window in zip(['before', 'after'], [before, after]):\n",
    "    path_save = path/R.name/mode\n",
    "    download_data(R, time_window, products, bands, path_save)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_event_data(event_id, year, coarse_mask_file, path=Path('.'), \n",
    "                   coarse_mask_doy_layer=1, products=['COPERNICUS/S2'], \n",
    "                   bands=['B4', 'B8', 'B12'], scale_factor=1e-4, composite_days=[60,60]):\n",
    "    rst_ba100 = open_tif(coarse_mask_file)\n",
    "    doy_start = rst_ba100.read(coarse_mask_doy_layer).min()\n",
    "    doy_end   = rst_ba100.read(coarse_mask_doy_layer).max()\n",
    "    time_start = pd.Timestamp(f'{year}-01-01') + pd.Timedelta(days=doy_start-1)\n",
    "    time_end   = pd.Timestamp(f'{year}-01-01') + pd.Timedelta(days=doy_end-1)\n",
    "    R = RegionST(event_id, list(rst_ba100.bounds), rst_ba100.transform[0], \n",
    "                 time_start=time_start, time_end=time_end, time_margin=1)\n",
    "    before = (R.times[0]-pd.Timedelta(days=composite_days[0]), R.times[0])\n",
    "    after  = (R.times[-1], R.times[-1]+pd.Timedelta(days=composite_days[1]))\n",
    "    for mode, time_window in zip(['before', 'after'], [before, after]):\n",
    "        path_save = path/R.name/mode\n",
    "        download_data(R, time_window, products, bands, path_save)\n",
    "\n",
    "    rst_ba100 = rst_ba100.read(coarse_mask_doy_layer)\n",
    "    s10before_files = np.array((path/R.name/'before').ls(exclude=['.xml']))[[1,2,0]].tolist()\n",
    "    s10after_files = np.array((path/R.name/'after').ls(exclude=['.xml']))[[1,2,0]].tolist()\n",
    "    transform = rasterio.open(str(s10before_files[0])).transform\n",
    "    crs = rasterio.open(str(s10before_files[0])).crs\n",
    "    rst_s10before = np.concatenate(\n",
    "        [rasterio.open(str(f)).read() for f in s10before_files]).astype(np.float16)*scale_factor\n",
    "    rst_s10after = np.concatenate(\n",
    "        [rasterio.open(str(f)).read() for f in s10after_files]).astype(np.float16)*scale_factor\n",
    "    rst_ba100 = downsample(rst_ba100, src_tfm=R.transform, dst_tfm=transform, \n",
    "                          dst_shape=(1, *rst_s10before.shape[-2:]), resampling='bilinear').astype(np.float32)\n",
    "    im = np.concatenate([rst_s10before, rst_s10after, rst_ba100], axis=0).transpose(1,2,0)\n",
    "    return im, transform, crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 449 ms, sys: 44.8 ms, total: 493 ms\n",
      "Wall time: 4.84 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((892, 881, 7),\n",
       " Affine(8.983152841195215e-05, 0.0, 6.579979793118671,\n",
       "        0.0, -8.983152841195215e-05, 43.21507371008099),\n",
       " CRS.from_epsg(4326))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "im, transform, crs = get_event_data('temp', 2020, 'temp/banet100m.tif')\n",
    "im.shape, transform, crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_models.ipynb.\n",
      "Converted 04_predict.ipynb.\n",
      "Converted 05_cli.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide \n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (banet_dev)",
   "language": "python",
   "name": "banet_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
